{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "# from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from linetimer import CodeTimer\n",
    "\n",
    "from utils import convert_ids_to_ordered\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxon/anaconda3/envs/torch_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "aspects = pd.read_csv('data/aspects.csv').set_index(\"aspect_id\")\n",
    "features = pd.read_csv('data/features.csv').set_index('feature_id')\n",
    "organizations = pd.read_csv('data/organisations.csv').set_index('org_id')\n",
    "reviews = pd.read_csv('data/reviews.csv')\n",
    "rubrics = pd.read_csv('data/rubrics.csv').set_index('rubric_id')\n",
    "test_users = pd.read_csv('data/test_users.csv').set_index('user_id')\n",
    "users = pd.read_csv('data/users.csv').set_index('user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews[reviews.rating.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['rating'] = reviews['rating'].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_ordered, orgs_ordered, reviews_ordered = convert_ids_to_ordered(users, organizations, reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split_day = 1050\n",
    "train_reviews = reviews_ordered[reviews_ordered.ts < validation_split_day]\n",
    "test_reviews = reviews_ordered[reviews_ordered.ts >= validation_split_day]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "P - latent vectors for clients  \n",
    "Q - latent vectors for organizations   \n",
    "R - ratings\n",
    "\n",
    "I minimize $||R - PQ^T||^2$ + Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 8\n",
    "Ps = torch.randn(len(users), latent_size) / latent_size + 1\n",
    "Qs = torch.randn(len(organizations), latent_size) / latent_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of the simplest baseline: 1.4542272189284848\n"
     ]
    }
   ],
   "source": [
    "mean_loss = np.mean((test_reviews.rating - train_reviews.rating.mean())**2)\n",
    "print(f\"Loss of the simplest baseline: {mean_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_array = reviews_ordered[['ordered_id_user', 'ordered_id_org', 'rating']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Iteration 0000000: Train loss 20.000163085289003\n",
      "Iteration 0100000: Train loss 12.60454071557804\n",
      "Iteration 0200000: Train loss 7.95053707435249\n",
      "Iteration 0300000: Train loss 5.614398251675314\n",
      "Iteration 0400000: Train loss 4.350297332322247\n",
      "Iteration 0500000: Train loss 3.6505616218140973\n",
      "Iteration 0600000: Train loss 3.2212150131995063\n",
      "Iteration 0700000: Train loss 2.942014774401203\n",
      "Iteration 0800000: Train loss 2.7506332762177603\n",
      "Iteration 0900000: Train loss 2.61221029781535\n",
      "Iteration 1000000: Train loss 2.4732513404492735\n",
      "Iteration 1100000: Train loss 2.3933822256187334\n",
      "Iteration 1200000: Train loss 2.3089722850372754\n",
      "Iteration 1300000: Train loss 2.216735360066854\n",
      "Iteration 1400000: Train loss 2.153378510641417\n",
      "Iteration 1500000: Train loss 2.0982054800993493\n",
      "Iteration 1600000: Train loss 2.0573318578633173\n",
      "Iteration 1700000: Train loss 2.0309325833584215\n",
      "Iteration 1800000: Train loss 1.970761494841691\n",
      "Iteration 1900000: Train loss 1.7820239596574619\n",
      "Iteration 2000000: Train loss 1.5083773844707624\n",
      "Iteration 2100000: Train loss 1.5155652998578444\n",
      "Iteration 2200000: Train loss 1.4846502013344287\n",
      "Iteration 2300000: Train loss 1.2957858519502552\n",
      "Iteration 2400000: Train loss 1.2328103251138158\n",
      "Iteration 2500000: Train loss 1.3166361843877623\n",
      "Iteration 2600000: Train loss 1.2931761976253133\n",
      "Iteration 2700000: Train loss 1.2467097600105443\n",
      "Iteration 2800000: Train loss 1.1401902094207266\n",
      "Iteration 2900000: Train loss 1.535573615022453\n",
      "Iteration 3000000: Train loss 1.6644043438710532\n",
      "Iteration 3100000: Train loss 1.7065641334140913\n",
      "Iteration 3200000: Train loss 1.7112295439523428\n",
      "Iteration 3300000: Train loss 1.7005199806209172\n",
      "Iteration 3400000: Train loss 1.6883811357926675\n",
      "Iteration 3500000: Train loss 1.6614023516019316\n",
      "Iteration 3600000: Train loss 1.6649639849718965\n",
      "Epoch 1\n",
      "Iteration 0000000: Train loss 1.6634718981708139\n",
      "Iteration 0100000: Train loss 1.5172082125986401\n",
      "Iteration 0200000: Train loss 1.4485518948790501\n",
      "Iteration 0300000: Train loss 1.4405985309960843\n",
      "Iteration 0400000: Train loss 1.4241872422754323\n",
      "Iteration 0500000: Train loss 1.4163675488934773\n",
      "Iteration 0600000: Train loss 1.4122850813751557\n",
      "Iteration 0700000: Train loss 1.411088772131331\n",
      "Iteration 0800000: Train loss 1.413229907625753\n",
      "Iteration 0900000: Train loss 1.4159898482624118\n",
      "Iteration 1000000: Train loss 1.4105153931589987\n",
      "Iteration 1100000: Train loss 1.412166663391522\n",
      "Iteration 1200000: Train loss 1.4096474841473732\n",
      "Iteration 1300000: Train loss 1.3933488710326207\n",
      "Iteration 1400000: Train loss 1.3906489535019977\n",
      "Iteration 1500000: Train loss 1.3879011097262146\n",
      "Iteration 1600000: Train loss 1.3895520863557635\n",
      "Iteration 1700000: Train loss 1.3951646737136896\n",
      "Iteration 1800000: Train loss 1.3795150124394744\n",
      "Iteration 1900000: Train loss 1.2587852216046291\n",
      "Iteration 2000000: Train loss 1.0695856094819178\n",
      "Iteration 2100000: Train loss 1.063460528768444\n",
      "Iteration 2200000: Train loss 1.0496717739676964\n",
      "Iteration 2300000: Train loss 0.9282356073778442\n",
      "Iteration 2400000: Train loss 0.9069863253208966\n",
      "Iteration 2500000: Train loss 0.9805178695327873\n",
      "Iteration 2600000: Train loss 0.9731181088132022\n",
      "Iteration 2700000: Train loss 0.9466209639411329\n",
      "Iteration 2800000: Train loss 0.8514873849479797\n",
      "Iteration 2900000: Train loss 1.1719163759979077\n",
      "Iteration 3000000: Train loss 1.2838887901777456\n",
      "Iteration 3100000: Train loss 1.3295328409679639\n",
      "Iteration 3200000: Train loss 1.3455145285267847\n",
      "Iteration 3300000: Train loss 1.3473750961338342\n",
      "Iteration 3400000: Train loss 1.3498900327906687\n",
      "Iteration 3500000: Train loss 1.3382792319931074\n",
      "Iteration 3600000: Train loss 1.3470088228801018\n",
      "Epoch 2\n",
      "Iteration 0000000: Train loss 1.3482726724210232\n",
      "Iteration 0100000: Train loss 1.2671185341555704\n",
      "Iteration 0200000: Train loss 1.2277082402091528\n",
      "Iteration 0300000: Train loss 1.2270391431895227\n",
      "Iteration 0400000: Train loss 1.2187899569499163\n",
      "Iteration 0500000: Train loss 1.2162851213717072\n",
      "Iteration 0600000: Train loss 1.217352547982057\n",
      "Iteration 0700000: Train loss 1.2202632321936362\n",
      "Iteration 0800000: Train loss 1.2233555980074988\n",
      "Iteration 0900000: Train loss 1.2270405537708482\n",
      "Iteration 1000000: Train loss 1.225285940215868\n",
      "Iteration 1100000: Train loss 1.2268770902309591\n",
      "Iteration 1200000: Train loss 1.2292336190160653\n",
      "Iteration 1300000: Train loss 1.2183565530507106\n",
      "Iteration 1400000: Train loss 1.218216805758934\n",
      "Iteration 1500000: Train loss 1.2176937172385893\n",
      "Iteration 1600000: Train loss 1.222022385235522\n",
      "Iteration 1700000: Train loss 1.2266691065463924\n",
      "Iteration 1800000: Train loss 1.2159239085789884\n",
      "Iteration 1900000: Train loss 1.1113763353609174\n",
      "Iteration 2000000: Train loss 0.9470267363610149\n",
      "Iteration 2100000: Train loss 0.9366283402237747\n",
      "Iteration 2200000: Train loss 0.9214602660170891\n",
      "Iteration 2300000: Train loss 0.816841144678473\n",
      "Iteration 2400000: Train loss 0.8037747444075459\n",
      "Iteration 2500000: Train loss 0.8689767564081247\n",
      "Iteration 2600000: Train loss 0.8630788318281349\n",
      "Iteration 2700000: Train loss 0.8410249591067114\n",
      "Iteration 2800000: Train loss 0.7516586964169214\n",
      "Iteration 2900000: Train loss 1.040952620797612\n",
      "Iteration 3000000: Train loss 1.1436450158100933\n",
      "Iteration 3100000: Train loss 1.1871214968602628\n",
      "Iteration 3200000: Train loss 1.2032873097510626\n",
      "Iteration 3300000: Train loss 1.2063982375967102\n",
      "Iteration 3400000: Train loss 1.2106876860232993\n",
      "Iteration 3500000: Train loss 1.202794202292244\n",
      "Iteration 3600000: Train loss 1.210984315729529\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "average_loss = 20\n",
    "for epoch in range(3):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    if average_loss < 10:\n",
    "        learning_rage = 0.05\n",
    "    for i, review in enumerate(reviews_array):\n",
    "        user_id, org_id, true_rating = review\n",
    "\n",
    "        pred_rating = Ps[user_id].dot(Qs[org_id])\n",
    "        error = pred_rating - true_rating\n",
    "        Ps_grad = learning_rate * error * Qs[org_id]\n",
    "        Qs_grad = learning_rate * error * Ps[user_id]\n",
    "        \n",
    "#         print(error.item(), Ps[user_id].item(), Qs[org_id].item(), Ps_grad.item(), Qs_grad.item())\n",
    "        Ps[user_id] -= Ps_grad\n",
    "        Qs[org_id] -= Qs_grad\n",
    "        \n",
    "        Ps[user_id][Ps[user_id] < 0] = 0.01\n",
    "        Qs[org_id][Qs[org_id] < 0] = 0.01\n",
    "        \n",
    "        loss = error ** 2\n",
    "        average_loss = 0.99999 * average_loss + 0.00001 * loss.item()\n",
    "        if i % 100000 == 0:\n",
    "            print(f\"Iteration {i:07d}: Train loss\", average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
