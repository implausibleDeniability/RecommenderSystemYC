{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from linetimer import CodeTimer\n",
    "\n",
    "from utils import convert_ids_to_ordered, MovingAverage\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxon/anaconda3/envs/core_ds/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "aspects = pd.read_csv('data/aspects.csv').set_index(\"aspect_id\")\n",
    "features = pd.read_csv('data/features.csv').set_index('feature_id')\n",
    "organizations = pd.read_csv('data/organisations.csv').set_index('org_id')\n",
    "reviews = pd.read_csv('data/reviews.csv')\n",
    "rubrics = pd.read_csv('data/rubrics.csv').set_index('rubric_id')\n",
    "test_users = pd.read_csv('data/test_users.csv').set_index('user_id')\n",
    "users = pd.read_csv('data/users.csv').set_index('user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews[reviews.rating.notna()]\n",
    "reviews['rating'] = reviews['rating'].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_ordered, orgs_ordered, reviews_ordered = convert_ids_to_ordered(users, organizations, reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split_day = 1050\n",
    "train_reviews = reviews_ordered[reviews_ordered.ts < validation_split_day]\n",
    "test_reviews = reviews_ordered[reviews_ordered.ts >= validation_split_day]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "P - latent vectors for clients  \n",
    "Q - latent vectors for organizations   \n",
    "R - ratings\n",
    "\n",
    "I minimize $||R - PQ^T||^2$ + Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of the simplest baseline: 1.4542272189284848\n"
     ]
    }
   ],
   "source": [
    "mean_loss = np.mean((test_reviews.rating - train_reviews.rating.mean())**2)\n",
    "print(f\"Loss of the simplest baseline: {mean_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews_array = train_reviews[['ordered_id_user', 'ordered_id_org', 'rating']].values\n",
    "test_reviews_array = test_reviews[['ordered_id_user', 'ordered_id_org', 'rating']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(\n",
    "        Ps: np.ndarray,\n",
    "        Qs: np.ndarray,\n",
    "        bias: float,\n",
    "    ) -> float:\n",
    "    \n",
    "    losses = []\n",
    "    for i, review in enumerate(test_reviews_array):\n",
    "        user_id, org_id, true_rating = review\n",
    "        pred_rating = Ps[user_id].dot(Qs[org_id]) + bias\n",
    "        error = pred_rating - true_rating\n",
    "        loss = error ** 2\n",
    "        losses.append(loss)\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        Ps: np.ndarray,\n",
    "        Qs: np.ndarray,\n",
    "        bias: float,\n",
    "        learning_rate: float = 0.01,\n",
    "        C: float = 0.0,\n",
    "        epochs: int = 7,\n",
    "        log_every: int = 1000000,\n",
    "    ) -> float:\n",
    "    \n",
    "    average_loss = MovingAverage(1e-6, 20)\n",
    "    for epoch in range(epochs):\n",
    "        for i, review in enumerate(train_reviews_array):\n",
    "            user_id, org_id, true_rating = review\n",
    "            \n",
    "            pred_rating = Ps[user_id].dot(Qs[org_id]) + bias\n",
    "            error = pred_rating - true_rating\n",
    "            Ps_grad = learning_rate * (error * Qs[org_id] + C * Ps[user_id])\n",
    "            Qs_grad = learning_rate * (error * Ps[user_id] + C * Qs[org_id])\n",
    "            bias_grad = learning_rate * error\n",
    "            \n",
    "            Ps[user_id] -= Ps_grad\n",
    "            Qs[org_id] -= Qs_grad\n",
    "            bias -= bias_grad\n",
    "\n",
    "            Qs[org_id][Qs[org_id] < 0] = 0.01\n",
    "\n",
    "            loss = error ** 2\n",
    "            average_loss.add(loss)\n",
    "            if i % log_every == 0:\n",
    "                print(f\"Iteration {i:07d}: Train loss\", average_loss)\n",
    "        print(f\"Test loss: {test_model(Ps, Qs, bias)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0000000: Train loss 19.99999390599476\n",
      "Iteration 1000000: Train loss 8.348709156612847\n",
      "Iteration 2000000: Train loss 3.8424197738816095\n",
      "Iteration 3000000: Train loss 2.247600869425811\n",
      "Test loss: 1.65428526271952\n",
      "\n",
      "Iteration 0000000: Train loss 2.216695394338302\n",
      "Iteration 1000000: Train loss 1.6306245272235604\n",
      "Iteration 2000000: Train loss 1.263067787516948\n",
      "Iteration 3000000: Train loss 1.2060928823406096\n",
      "Test loss: 1.6365496724923827\n",
      "\n",
      "Iteration 0000000: Train loss 1.2086355352504634\n",
      "Iteration 1000000: Train loss 1.1948853229617484\n"
     ]
    }
   ],
   "source": [
    "latent_size = 4\n",
    "Ps = np.random.randn(len(users), latent_size) / latent_size + 1\n",
    "Qs = np.random.randn(len(organizations), latent_size) / latent_size + 1\n",
    "bias = 4 + np.random.randn()\n",
    "train_model(Ps, Qs, bias, C=0.1, epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "core_ds",
   "language": "python",
   "name": "core_ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
